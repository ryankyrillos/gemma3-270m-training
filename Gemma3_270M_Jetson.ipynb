{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f65448",
   "metadata": {},
   "source": [
    "# Gemma 3 270M Training on NVIDIA Jetson Orin Nano\n",
    "\n",
    "This notebook is optimized for running locally on an NVIDIA Jetson Orin Nano device. It includes adjustments for limited resources (8GB RAM) and assumes the dataset is in the repository root.\n",
    "\n",
    "## Prerequisites\n",
    "- JetPack 6.x installed on Jetson\n",
    "- PyTorch installed with CUDA support\n",
    "- Dataset file: `net_5ghz_sft.jsonl` in the repository root\n",
    "\n",
    "## Setup Instructions\n",
    "1. Clone the repository: `git clone https://github.com/ryankyrillos/gemma3-270m-training.git`\n",
    "2. Navigate to the directory: `cd gemma3-270m-training`\n",
    "3. Ensure `net_5ghz_sft.jsonl` is in the root directory\n",
    "4. Run: `jupyter notebook`\n",
    "5. Open this notebook and execute cells in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7935d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run if not already installed)\n",
    "!pip install transformers datasets tiktoken tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "# Check device availability\n",
    "device = \"cpu\"  # Forced to CPU since CUDA is not available\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cpu\":\n",
    "    print(f\"CPU cores available: {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63349ad9",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Dataset\n",
    "\n",
    "The dataset is loaded from the local repository directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from repository root\n",
    "dataset_path = \"./net_5ghz_sft.jsonl\"\n",
    "if os.path.exists(dataset_path):\n",
    "    ds = load_dataset(\"json\", data_files=dataset_path)\n",
    "    ds = ds['train'].train_test_split(test_size=0.1)\n",
    "    ds['validation'] = ds.pop('test')\n",
    "    print(\"Dataset loaded successfully\")\n",
    "    print(f\"Train size: {len(ds['train'])}, Validation size: {len(ds['validation'])}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {dataset_path}. Please ensure net_5ghz_sft.jsonl is in the repository root.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a57a55",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize Dataset\n",
    "\n",
    "Tokenize the dataset and create binary files for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def process(example):\n",
    "    messages = example['messages']\n",
    "    text = \"\"\n",
    "    for msg in messages:\n",
    "        text += msg['role'] + \": \" + msg['content'] + \"\\n\"\n",
    "    ids = enc.encode_ordinary(text)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['messages'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=4,  # Reduced for Jetson\n",
    "    )\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 512  # Reduced for Jetson\n",
    "        \n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()\n",
    "    print(\"Tokenization completed\")\n",
    "else:\n",
    "    print(\"Binary files already exist, skipping tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae53fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and validation data\n",
    "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy(data[i:i+block_size].astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy(data[i+1:i+1+block_size].astype(np.int64)) for i in ix])\n",
    "    return x, y\n",
    "\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'validation']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d8fe2",
   "metadata": {},
   "source": [
    "## Step 3: Define Model Architecture\n",
    "\n",
    "Gemma 3 270M model definition optimized for Jetson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-6, bias=False):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.zeros(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * (self.scale + 1) / (x.norm(2, dim=-1, keepdim=True) + self.eps) + (self.shift if self.shift is not None else 0)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(cfg[\"emb_dim\"], 3 * cfg[\"emb_dim\"])\n",
    "        self.proj = nn.Linear(cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        self.ff = nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"])\n",
    "        self.ff_proj = nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.attn(self.norm1(x))\n",
    "        q, k, v = qkv.split(C, dim=-1)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        x = x + self.proj(y)\n",
    "        x = x + self.ff_proj(F.gelu(self.ff(self.norm2(x))))\n",
    "        return x\n",
    "\n",
    "class Gemma3Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=cfg[\"norm_eps\"])\n",
    "        self.lm_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        tok_emb = self.tok_emb(idx)\n",
    "        x = tok_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = float(\"-inf\")\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "GEMMA3_CONFIG_270M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"emb_dim\": 512,\n",
    "    \"n_layers\": 6,\n",
    "    \"n_heads\": 8,\n",
    "    \"head_dim\": 64,\n",
    "    \"context_length\": 1024,\n",
    "    \"norm_eps\": 1e-6,\n",
    "    \"rope_base\": 10000,\n",
    "    \"dtype\": torch.float32\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = Gemma3Model(GEMMA3_CONFIG_270M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601aaedb",
   "metadata": {},
   "source": [
    "## Step 4: Training Configuration\n",
    "\n",
    "Optimized parameters for Jetson Orin Nano's limited resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config - Optimized for CPU on Jetson\n",
    "learning_rate = 1e-4\n",
    "max_iters = 100  # Further reduced for testing\n",
    "warmup_steps = 10\n",
    "min_lr = 5e-4\n",
    "eval_iters = 10\n",
    "batch_size = 1  # Reduced for CPU\n",
    "block_size = 16  # Reduced for CPU\n",
    "gradient_accumulation_steps = 2  # Reduced for CPU\n",
    "\n",
    "device = \"cpu\"\n",
    "device_type = 'cpu'\n",
    "dtype = 'float32'  # Use float32 for CPU\n",
    "ptdtype = torch.float32\n",
    "ctx = nullcontext()\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc4c43",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "Execute the training loop with progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=warmup_steps)\n",
    "scheduler_decay = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters - warmup_steps, eta_min=min_lr)\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps])\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list.append(losses['train'])\n",
    "        validation_loss_list.append(losses['val'])\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "    \n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    \n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "    \n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a9c41",
   "metadata": {},
   "source": [
    "## Step 6: Generate Text\n",
    "\n",
    "Test the trained model by generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feca5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(best_model_params_path))\n",
    "model.eval()\n",
    "\n",
    "# Generate sample text\n",
    "idx = torch.tensor([[enc.encode(\"System: You are a wireless network engineer.\")]], dtype=torch.long).to(device)\n",
    "generated = model.generate(idx, max_new_tokens=50, temperature=0.8)\n",
    "generated_text = enc.decode(generated[0].tolist())\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
