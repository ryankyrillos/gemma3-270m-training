{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f65448",
   "metadata": {},
   "source": [
    "# Gemma 3 270M Training on NVIDIA Jetson Orin Nano\n",
    "\n",
    "This notebook is optimized for running locally on an NVIDIA Jetson Orin Nano device. It includes adjustments for limited resources (8GB RAM) and assumes the dataset is in the repository root.\n",
    "\n",
    "## Prerequisites\n",
    "- JetPack 6.x installed on Jetson\n",
    "- PyTorch installed with CUDA support\n",
    "- Dataset file: `net_5ghz_sft.jsonl` in the repository root\n",
    "\n",
    "## Setup Instructions\n",
    "1. Clone the repository: `git clone https://github.com/ryankyrillos/gemma3-270m-training.git`\n",
    "2. Navigate to the directory: `cd gemma3-270m-training`\n",
    "3. Ensure `net_5ghz_sft.jsonl` is in the root directory\n",
    "4. Run: `jupyter notebook`\n",
    "5. Open this notebook and execute cells in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7935d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run if not already installed)\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install transformers datasets tiktoken tqdm numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63349ad9",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare Dataset\n",
    "\n",
    "The dataset is loaded from the local repository directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade7c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from repository root\n",
    "dataset_path = \"./net_5ghz_sft.jsonl\"\n",
    "if os.path.exists(dataset_path):\n",
    "    ds = load_dataset(\"json\", data_files=dataset_path)\n",
    "    ds = ds['train'].train_test_split(test_size=0.1)\n",
    "    ds['validation'] = ds.pop('test')\n",
    "    print(\"Dataset loaded successfully\")\n",
    "    print(f\"Train size: {len(ds['train'])}, Validation size: {len(ds['validation'])}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {dataset_path}. Please ensure net_5ghz_sft.jsonl is in the repository root.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a57a55",
   "metadata": {},
   "source": [
    "## Step 2: Tokenize Dataset\n",
    "\n",
    "Tokenize the dataset and create binary files for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def process(example):\n",
    "    messages = example['messages']\n",
    "    text = \"\"\n",
    "    for msg in messages:\n",
    "        text += msg['role'] + \": \" + msg['content'] + \"\\n\"\n",
    "    ids = enc.encode_ordinary(text)\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['messages'],\n",
    "        desc=\"tokenizing the splits\",\n",
    "        num_proc=4,  # Reduced for Jetson\n",
    "    )\n",
    "    for split, dset in tokenized.items():\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "        filename = f'{split}.bin'\n",
    "        dtype = np.uint16\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "        total_batches = 512  # Reduced for Jetson\n",
    "        \n",
    "        idx = 0\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "            arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "        arr.flush()\n",
    "    print(\"Tokenization completed\")\n",
    "else:\n",
    "    print(\"Binary files already exist, skipping tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6d8fe2",
   "metadata": {},
   "source": [
    "## Step 3: Define Model Architecture\n",
    "\n",
    "Gemma 3 270M model definition optimized for Jetson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition (simplified for brevity - include full Gemma3Model class here)\n",
    "# ... (Insert the full model code from the original notebook)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = Gemma3Model(GEMMA3_CONFIG_270M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601aaedb",
   "metadata": {},
   "source": [
    "## Step 4: Training Configuration\n",
    "\n",
    "Optimized parameters for Jetson Orin Nano's limited resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Config - Optimized for Jetson\n",
    "learning_rate = 1e-4\n",
    "max_iters = 50000  # Reduced for testing\n",
    "warmup_steps = 500\n",
    "min_lr = 5e-4\n",
    "eval_iters = 200\n",
    "batch_size = 4  # Reduced for Jetson\n",
    "block_size = 64  # Reduced for Jetson\n",
    "gradient_accumulation_steps = 8  # Reduced for Jetson\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc4c43",
   "metadata": {},
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "Execute the training loop with progress monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LinearLR(optimizer, total_iters=warmup_steps)\n",
    "scheduler_decay = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters - warmup_steps, eta_min=min_lr)\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps])\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list.append(losses['train'])\n",
    "        validation_loss_list.append(losses['val'])\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "    \n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    \n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a9c41",
   "metadata": {},
   "source": [
    "## Step 6: Generate Text\n",
    "\n",
    "Test the trained model by generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feca5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(best_model_params_path))\n",
    "model.eval()\n",
    "\n",
    "# Generate sample text\n",
    "idx = torch.tensor([[enc.encode(\"System: You are a wireless network engineer.\")]], dtype=torch.long).to(device)\n",
    "generated = model.generate(idx, max_new_tokens=50, temperature=0.8)\n",
    "generated_text = enc.decode(generated[0].tolist())\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
